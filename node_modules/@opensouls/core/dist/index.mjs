// src/index.ts
import { z } from "zod";

// src/cognitiveStep.ts
var createCognitiveStep = (transformationOptionsGenerator) => {
  return async (workingMemory, singleArg, opts = {}) => {
    const transformOpts = transformationOptionsGenerator(singleArg);
    return workingMemory.transform(transformOpts, opts);
  };
};

// src/processors/OpenAIProcessor.ts
import OpenAI from "openai";
import { encodeChatGenerator, encodeGenerator } from "gpt-tokenizer/model/gpt-4";
import { trace, context } from "@opentelemetry/api";
import { backOff } from "exponential-backoff";
import { fromZodError } from "zod-validation-error";

// src/processors/registry.ts
var processorRegistry = {};
function registerProcessor(name, processor) {
  if (processorRegistry[name]) {
    throw new Error(`Processor with name ${name} already exists`);
  }
  processorRegistry[name] = processor;
}
function getProcessor(name, opts) {
  if (!processorRegistry[name]) {
    throw new Error(`Processor with name ${name} does not exist`);
  }
  return processorRegistry[name](opts);
}

// src/Memory.ts
var ChatMessageRoleEnum = /* @__PURE__ */ ((ChatMessageRoleEnum2) => {
  ChatMessageRoleEnum2["System"] = "system";
  ChatMessageRoleEnum2["User"] = "user";
  ChatMessageRoleEnum2["Assistant"] = "assistant";
  ChatMessageRoleEnum2["Function"] = "function";
  return ChatMessageRoleEnum2;
})(ChatMessageRoleEnum || {});

// src/processors/Processor.ts
var textFromContent = (content) => {
  if (typeof content === "string") {
    return content;
  }
  const textContent = content.find((c) => c.type === "text");
  if (!textContent) {
    return "";
  }
  return textContent.text;
};
var JSON_MESSAGE = "You only speak JSON. Respond only with JSON properly conforming to the provided schema (but not returning the schema itself), with no other content.";
var prepareMemoryForJSON = (workingMemory, jsonMessage = JSON_MESSAGE) => {
  const systemMem = workingMemory.find((memory) => memory.role === "system" /* System */);
  if (systemMem && textFromContent(systemMem.content).includes("JSON")) {
    return workingMemory;
  }
  if (systemMem) {
    return workingMemory.map((memory) => {
      if (memory._id === systemMem._id) {
        return {
          ...memory,
          content: systemMem.content + " \n\n " + jsonMessage
        };
      }
      return memory;
    });
  }
  return workingMemory.prepend([{
    role: "system" /* System */,
    content: jsonMessage
  }]);
};
function extractJSON(str) {
  if (!str)
    return null;
  const jsonStart = str.indexOf("{");
  if (jsonStart === -1)
    return null;
  for (let i = jsonStart; i < str.length; i++) {
    if (str[i] === "}") {
      const potentialJson = str.slice(jsonStart, i + 1);
      try {
        JSON.parse(potentialJson);
        return potentialJson;
      } catch (e) {
      }
    }
  }
  return null;
}

// src/processors/messageRoleFixer.ts
var fixMessageRoles = (fixMethods, messages) => {
  if (!fixMethods.singleSystemMessage && !fixMethods.forcedRoleAlternation) {
    return messages;
  }
  let newMessages = messages;
  if (fixMethods.singleSystemMessage) {
    newMessages = messages.map((originalMessage, i) => {
      const message = { ...originalMessage };
      if (i === 0) {
        return message;
      }
      if (message.role === "system" /* System */) {
        message.role = "user" /* User */;
        return message;
      }
      return message;
    });
  }
  if (fixMethods.forcedRoleAlternation) {
    let lastRole;
    const { messages: messages2 } = newMessages.reduce((acc, message) => {
      if (lastRole !== message.role) {
        acc.messages.push(message);
        lastRole = message.role;
        acc.grouped = [message.content];
      } else {
        const lastMessage = acc.messages[acc.messages.length - 1];
        acc.grouped.push(message.content);
        lastMessage.content = mergeContent(acc.grouped);
      }
      return acc;
    }, { messages: [], grouped: [] });
    newMessages = messages2;
    if (newMessages[0]?.role === "assistant" /* Assistant */) {
      newMessages.unshift({
        content: "...",
        role: "user" /* User */
      });
    }
  }
  return newMessages;
};
var extractTextFromContent = (content) => {
  if (Array.isArray(content)) {
    return content.map((c) => {
      if (c.type === "text") {
        return c.text;
      }
      return "";
    }).join("\n");
  }
  return content || "";
};
var extractImageFromContent = (content) => {
  if (Array.isArray(content)) {
    return content.filter((c) => c.type === "image_url");
  }
  return [];
};
var mergeContent = (messages) => {
  const newContent = [
    {
      type: "text",
      text: ""
    }
  ];
  for (const message of messages) {
    const txt = extractTextFromContent(message);
    const images = extractImageFromContent(message);
    newContent.push(...images);
    newContent[0].text += txt + "\n\n";
  }
  return newContent;
};

// src/utils.ts
import { codeBlock } from "common-tags";
var indentNicely = codeBlock;
var stripEntityAndVerb = (soulName, _verb, response) => {
  let strippedResponse = response.replace(new RegExp(`${soulName}.*?:`, "i"), "").trim();
  strippedResponse = strippedResponse.replace(/^["']|["']$/g, "").trim();
  return strippedResponse;
};
var stripEntityAndVerbFromStream = async ({ soulName }, stream) => {
  const prefix = new RegExp(`^${soulName}.*?:\\s*["']*`, "i");
  const suffix = /["']$/;
  let isStreaming = !prefix;
  let prefixMatched = !prefix;
  let buffer = "";
  const isStreamingBuffer = [];
  const processedStream = async function* () {
    for await (const chunk of stream) {
      if (isStreaming) {
        if (!suffix) {
          yield chunk;
          continue;
        }
        isStreamingBuffer.push(chunk);
        if (isStreamingBuffer.length > 2) {
          yield isStreamingBuffer.shift();
        }
        continue;
      }
      buffer += chunk;
      if (prefix && prefix.test(buffer)) {
        if (prefixMatched) {
          isStreaming = true;
          buffer = buffer.replace(prefix, "");
          yield buffer;
          buffer = "";
          continue;
        }
        prefixMatched = true;
      }
    }
    buffer = [buffer, ...isStreamingBuffer].join("");
    if (!isStreaming && prefix) {
      buffer = buffer.replace(prefix, "");
    }
    if (buffer.length > 0) {
      if (suffix) {
        buffer = buffer.replace(suffix, "");
        yield buffer;
        return;
      }
      yield buffer;
    }
  }();
  return processedStream;
};

// src/forkStream.ts
import { ReadableStream } from "web-streams-polyfill";
function forkStream(originalStream, count = 2) {
  const streams = Array.from({ length: count }, () => {
    let controller = { current: null };
    const stream = new ReadableStream({
      start(c) {
        controller.current = c;
      },
      cancel() {
        console.log("Stream was cancelled.");
      }
    });
    return {
      stream,
      controller
    };
  });
  const processStream = async () => {
    try {
      for await (const chunk of originalStream) {
        streams.forEach(({ stream, controller }) => {
          if (controller.current) {
            controller.current.enqueue(chunk);
          }
        });
      }
      streams.forEach(({ stream, controller }) => {
        if (controller.current) {
          controller.current.close();
        }
      });
    } catch (err) {
      console.error("Error processing stream:", err);
      streams.forEach(({ stream, controller }) => {
        if (controller.current) {
          controller.current.error(err);
        }
      });
    }
  };
  processStream();
  return streams.map(({ stream }) => stream);
}

// src/processors/OpenAIProcessor.ts
var tracer = trace.getTracer(
  "open-souls-OpenAIProcessor",
  "0.0.1"
);
var tokenLength = (messagesOrContent) => {
  let tokenCount = 0;
  if (typeof messagesOrContent === "string") {
    for (const tokens of encodeGenerator(messagesOrContent)) {
      tokenCount += tokens.length;
    }
    return tokenCount;
  }
  const messagesWithoutImages = messagesOrContent.map((m) => {
    if (!Array.isArray(m.content)) {
      return m;
    }
    const text = m.content.find((c) => c.type === "text");
    const images = m.content.filter((c) => c.type === "image_url");
    tokenCount += images.length * 765;
    return {
      ...m,
      content: text?.text || ""
    };
  });
  for (const tokens of encodeChatGenerator(messagesWithoutImages)) {
    tokenCount += tokens.length;
  }
  return tokenCount;
};
var memoryToChatMessage = (memory) => {
  return {
    role: memory.role,
    content: memory.content,
    ...memory.name && { name: memory.name }
  };
};
async function* chunkStreamToTextStream(chunkStream) {
  try {
    for await (const chunk of chunkStream) {
      yield chunk.choices[0].delta.content || "";
    }
  } catch (err) {
    console.error("chunkStreamToTextStream error", err);
    throw err;
  }
}
var DEFAULT_MODEL = "gpt-3.5-turbo-0125";
var OpenAIProcessor = class {
  static {
    this.label = "openai";
  }
  constructor({ clientOptions, singleSystemMessage, forcedRoleAlternation, defaultRequestOptions, defaultCompletionParams, disableResponseFormat }) {
    this.client = new OpenAI(clientOptions);
    this.singleSystemMessage = singleSystemMessage || false;
    this.forcedRoleAlternation = forcedRoleAlternation || false;
    this.defaultRequestOptions = defaultRequestOptions || {};
    this.disableResponseFormat = disableResponseFormat || false;
    this.defaultCompletionParams = defaultCompletionParams || {};
  }
  async process(opts) {
    return tracer.startActiveSpan("OpenAIProcessor.process", async (span) => {
      try {
        context.active();
        let memory = opts.memory;
        if (opts.schema) {
          memory = prepareMemoryForJSON(memory);
        }
        span.setAttributes({
          processOptions: JSON.stringify(opts),
          memory: JSON.stringify(memory)
        });
        return backOff(
          async () => {
            const resp = await this.execute({
              ...opts,
              memory
            });
            if (opts.schema) {
              const completion = await resp.rawCompletion;
              const extracted = extractJSON(completion);
              span.addEvent("extracted");
              span.setAttribute("extracted", extracted || "none");
              if (!extracted) {
                console.error("no json found in completion", completion);
                throw new Error("no json found in completion");
              }
              try {
                const parsed = opts.schema.parse(JSON.parse(extracted));
                span.addEvent("parsed");
                span.end();
                return {
                  ...resp,
                  parsed: Promise.resolve(parsed)
                };
              } catch (err) {
                span.recordException(err);
                const zodError = fromZodError(err);
                console.log("zod error", zodError.toString());
                memory = memory.concat([
                  {
                    role: "assistant" /* Assistant */,
                    content: extracted
                  },
                  {
                    role: "user" /* User */,
                    content: indentNicely`
                      ## JSON Errors
                      ${zodError.toString()}.
                      
                      Please fix the error(s) and try again, conforming exactly to the provided JSON schema.
                    `
                  }
                ]);
                throw err;
              }
            }
            return {
              ...resp,
              parsed: resp.rawCompletion
            };
          },
          {
            numOfAttempts: 5,
            retry: (err) => {
              if (err.message.includes("aborted")) {
                return false;
              }
              span.addEvent("retry");
              console.error("retrying due to error", err);
              return true;
            }
          }
        );
      } catch (err) {
        console.error("error in process", err);
        span.recordException(err);
        span.end();
        throw err;
      }
    });
  }
  async execute({
    maxTokens,
    memory,
    model: developerSpecifiedModel,
    schema,
    signal,
    timeout,
    temperature
  }) {
    return tracer.startActiveSpan("OpenAIProcessor.execute", async (span) => {
      try {
        const model = developerSpecifiedModel || this.defaultCompletionParams.model || DEFAULT_MODEL;
        const messages = this.possiblyFixMessageRoles(memory.memories.map(memoryToChatMessage));
        const params = {
          ...this.defaultCompletionParams,
          ...maxTokens && { max_tokens: maxTokens },
          model,
          messages,
          temperature: temperature || 0.8,
          stream: true
        };
        span.setAttributes({
          outgoingParams: JSON.stringify(params)
        });
        const stream = await this.client.chat.completions.create(
          {
            ...params,
            stream: true,
            ...!this.disableResponseFormat && { response_format: { type: schema ? "json_object" : "text" } }
          },
          {
            ...this.defaultRequestOptions,
            signal,
            timeout: timeout || 1e4
          }
        );
        const [textStream1, textStream2] = forkStream(chunkStreamToTextStream(stream), 2);
        const fullContentPromise = new Promise(async (resolve, reject) => {
          try {
            let fullText = "";
            for await (const message of textStream1) {
              span.addEvent("chunk", { length: message.length });
              fullText += message;
            }
            span.setAttribute("response", fullText);
            resolve(fullText);
          } catch (err) {
            reject(err);
          }
        });
        const usagePromise = new Promise(async (resolve, reject) => {
          try {
            const fullContent = await fullContentPromise;
            const [inputTokenCount, outputTokenCount] = await Promise.all([
              tokenLength(messages),
              tokenLength(fullContent)
            ]);
            span.setAttribute("model", model);
            span.setAttribute("usage-input", inputTokenCount);
            span.setAttribute("usage-output", outputTokenCount);
            resolve({
              model,
              input: inputTokenCount,
              output: outputTokenCount
            });
          } catch (err) {
            reject(err);
          } finally {
            span.end();
          }
        });
        return {
          rawCompletion: fullContentPromise,
          stream: textStream2,
          usage: usagePromise
        };
      } catch (err) {
        span.recordException(err);
        span.end();
        throw err;
      }
    });
  }
  possiblyFixMessageRoles(messages) {
    return fixMessageRoles({ singleSystemMessage: this.singleSystemMessage, forcedRoleAlternation: this.forcedRoleAlternation }, messages);
  }
};
registerProcessor(OpenAIProcessor.label, (opts = {}) => new OpenAIProcessor(opts));

// src/processors/AnthropicProcessor.ts
import Anthropic from "@anthropic-ai/sdk";
import { trace as trace2, context as context2 } from "@opentelemetry/api";
import { backOff as backOff2 } from "exponential-backoff";
var tracer2 = trace2.getTracer(
  "open-souls-OpenAIProcessor",
  "0.0.1"
);
var memoryToChatMessage2 = (memory) => {
  return {
    role: memory.role,
    content: memory.content,
    ...memory.name && { name: memory.name }
  };
};
var openAiToAnthropicMessages = (openAiMessages) => {
  let systemMessage;
  const messages = openAiMessages.map((m) => {
    if (m.role === "system" /* System */) {
      if (openAiMessages.length > 1) {
        systemMessage ||= "";
        systemMessage += m.content + "\n";
        return void 0;
      }
      return {
        content: m.content,
        role: "user" /* User */
      };
    }
    return {
      content: m.content,
      role: m.role
    };
  }).filter(Boolean);
  if (messages[0]?.role === "assistant" /* Assistant */) {
    messages.unshift({
      content: "...",
      role: "user" /* User */
    });
  }
  return { system: systemMessage, messages };
};
async function* chunkStreamToTextStream2(chunkStream) {
  try {
    for await (const evt of chunkStream) {
      if (evt.type !== "content_block_delta") {
        continue;
      }
      yield evt.delta.text;
    }
  } catch (err) {
    if (err.message?.toLowerCase().includes("abort")) {
      return;
    }
    throw err;
  }
}
async function chunkStreamToUsage(chunkStream) {
  const usage = { input: 0, output: 0 };
  for await (const evt of chunkStream) {
    if (evt.type === "message_start") {
      usage.input = evt.message.usage.input_tokens;
    }
    if (evt.type === "message_delta" && evt.usage) {
      usage.output = evt.usage.output_tokens;
    }
  }
  return usage;
}
var DEFAULT_MODEL2 = "claude-3-opus-20240229";
var AnthropicProcessor = class {
  static {
    this.label = "anthropic";
  }
  constructor({ clientOptions, defaultRequestOptions, defaultCompletionParams, customClient }) {
    this.client = new (customClient ?? Anthropic)(clientOptions);
    this.defaultRequestOptions = defaultRequestOptions || {};
    this.defaultCompletionParams = defaultCompletionParams || {};
  }
  async process(opts) {
    return tracer2.startActiveSpan("OpenAIProcessor.process", async (span) => {
      context2.active();
      let memory = opts.memory;
      if (opts.schema) {
        memory = prepareMemoryForJSON(memory);
      }
      span.setAttributes({
        processOptions: JSON.stringify(opts),
        memory: JSON.stringify(memory)
      });
      return backOff2(
        async () => {
          const resp = await this.execute({
            ...opts,
            memory
          });
          if (opts.schema) {
            const completion = await resp.rawCompletion;
            const extracted = extractJSON(completion);
            span.addEvent("extracted");
            span.setAttribute("extracted", extracted || "none");
            if (!extracted) {
              throw new Error("no json found in completion");
            }
            const parsed = opts.schema.parse(JSON.parse(extracted));
            span.addEvent("parsed");
            span.end();
            return {
              ...resp,
              parsed: Promise.resolve(parsed)
            };
          }
          return {
            ...resp,
            parsed: resp.rawCompletion
          };
        },
        {
          numOfAttempts: 5,
          retry: (err) => {
            if (err.message.includes("aborted")) {
              return false;
            }
            span.addEvent("retry");
            console.error("retrying due to error", err);
            return true;
          }
        }
      );
    });
  }
  async execute({
    maxTokens,
    memory,
    model: developerSpecifiedModel,
    signal,
    timeout,
    temperature
  }) {
    return tracer2.startActiveSpan("AnthropicProcessor.execute", async (span) => {
      try {
        const model = developerSpecifiedModel || this.defaultCompletionParams.model || DEFAULT_MODEL2;
        const { system, messages } = openAiToAnthropicMessages(this.possiblyFixMessageRoles(memory.memories.map(memoryToChatMessage2)));
        const params = {
          system,
          max_tokens: maxTokens || this.defaultCompletionParams.max_tokens || 512,
          model,
          messages,
          temperature: temperature || 0.8
        };
        span.setAttributes({
          outgoingParams: JSON.stringify(params)
        });
        const stream = this.client.messages.stream(
          {
            ...this.defaultCompletionParams,
            ...params
          },
          {
            ...this.defaultRequestOptions,
            signal,
            timeout: timeout || 1e4
          }
        );
        const [baseStream1, baseStream2] = forkStream(stream, 2);
        const [textStream1, textStream2] = forkStream(chunkStreamToTextStream2(baseStream1), 2);
        const fullContentPromise = new Promise(async (resolve, reject) => {
          try {
            let fullText = "";
            for await (const message of textStream1) {
              span.addEvent("chunk", { length: message.length });
              fullText += message;
            }
            span.setAttribute("response", fullText);
            resolve(fullText);
          } catch (err) {
            reject(err);
          }
        });
        const usagePromise = new Promise(async (resolve, reject) => {
          try {
            const { input: inputTokenCount, output: outputTokenCount } = await chunkStreamToUsage(baseStream2);
            span.setAttribute("model", model);
            span.setAttribute("usage-input", inputTokenCount);
            span.setAttribute("usage-output", outputTokenCount);
            resolve({
              model,
              input: inputTokenCount,
              output: outputTokenCount
            });
          } catch (err) {
            reject(err);
          } finally {
            span.end();
          }
        });
        return {
          rawCompletion: fullContentPromise,
          stream: textStream2,
          usage: usagePromise
        };
      } catch (err) {
        span.recordException(err);
        throw err;
      }
    });
  }
  possiblyFixMessageRoles(messages) {
    return fixMessageRoles({ singleSystemMessage: true, forcedRoleAlternation: true }, messages);
  }
};
registerProcessor(AnthropicProcessor.label, (opts = {}) => new AnthropicProcessor(opts));

// src/WorkingMemory.ts
import { nanoid } from "nanoid";
import { EventEmitter } from "eventemitter3";
import { zodToJsonSchema } from "zod-to-json-schema";
var defaultPostProcessor = (_workingMemory, response) => {
  const memory = {
    role: "assistant" /* Assistant */,
    content: response.toString()
  };
  return [memory, response];
};
var pendingFactory = () => {
  const pending = {};
  return () => pending;
};
var usageFactory = () => {
  const usage = {
    model: "",
    input: 0,
    output: 0
  };
  return () => usage;
};
var memoryFactory = (initialMemories) => {
  const memories = [...initialMemories || []];
  return () => memories;
};
var WorkingMemory = class _WorkingMemory extends EventEmitter {
  constructor({ soulName, memories, postCloneTransformation, processor }) {
    super();
    this.processor = Object.freeze({
      name: "openai"
    });
    this.id = nanoid();
    this._memories = memoryFactory(this.memoriesFromInputMemories(memories || []));
    this.soulName = soulName;
    if (processor) {
      this.processor = processor;
    }
    this._pending = pendingFactory();
    this._postCloneTransformation = postCloneTransformation || ((workingMemory) => workingMemory);
    this._usage = usageFactory();
  }
  /**
   * Gets the usage information of input/output tokens for the current WorkingMemory instance.
   * This information is only available once the WorkingMemory is no longer pending and after a transformation has been performed.
   * 
   * @returns An object containing the model name, and the number of input and output tokens used.
   * 
   * @example
   * ```
   * const usageInfo = workingMemory.usage;
   * console.log(`Model: ${usageInfo.model}, Input Tokens: ${usageInfo.input}, Output Tokens: ${usageInfo.output}`);
   * ```
   */
  get usage() {
    return { ...this._usage() };
  }
  get memories() {
    return [...this.internalMemories];
  }
  /**
   * The `length` attribute returns the number of memories currently stored in the WorkingMemory instance.
  * 
   * @returns The total number of memories.
   * 
   * @example
   * ```
   * const workingMemory = new WorkingMemory({ soulName: 'example' });
   * console.log(workingMemory.length); // Outputs 0 (no memories there)
   * ```
   */
  get length() {
    return this.internalMemories.length;
  }
  get internalMemories() {
    return this._memories();
  }
  /**
   * Retrieves a memory at a specified index from the internal memories array.
   * 
   * @param index - The zero-based index of the memory to retrieve.
   * @returns The memory object at the specified index, or undefined if the index is out of bounds.
   * 
   * @example
   * ```
   * const memoryAtIndex = workingMemory.at(1);
   * if (memoryAtIndex) {
   *   console.log(`Memory at index 1:`, memoryAtIndex);
   * } else {
   *   console.log(`No memory found at index 1.`);
   * }
   * ```
   */
  at(index) {
    return this.internalMemories[index];
  }
  /**
   * The `finished` attribute returns a promise which resolves once the current pending transformation using a CognitiveStep is complete.
   * This is a fairly low level API and most users will not need to worry about this, since working memory uses this attribute internally and
   * the soul-engine does as well.
   * 
   * Only streaming cognitive functions will result in WorkingMemory with pending transformations.
   * 
   * @returns A promise that resolves once the current pending transformation is finished.
   * 
   * @example
   * ```
   * const [workingMemory, stream] = await cognitiveStep(workingMemory, userArgs, { stream: true });
   * await workingMemory.finished;
   * console.log('Transformation complete.');
   * ```
   * 
   * @example
   * ```
   * [workingMemory, stream] = await cognitiveStep(workingMemory, userArgs, { stream: true });
   * // even though we are not awaiting workingMemory.finished it's ok and will be automatically awaited.
   * [workingMemory] = await cognitiveStep(workingMemory, userArgs);
   * // all transformations are complete here.
   * ```
   */
  get finished() {
    const pendingObj = this._pending();
    if (!pendingObj.pending) {
      return Promise.resolve();
    }
    return pendingObj.pending;
  }
  /**
   * Creates a clone of the current WorkingMemory instance, optionally replacing its memories with new ones.
   * 
   * @param replacementMemories - An optional array of InputMemory objects to replace the current memories in the clone.
   *                              If not provided, the clone will retain the original memories.
   * @returns A new WorkingMemory instance, with optionally replaced memories.
   * 
   * @example
   * ```
   * const originalMemory = new WorkingMemory({ soulName: "ExampleSoul", memories: [{...memory}] });
   * const clonedMemory = originalMemory.clone([optionalNewMemories]);
   * ```
   */
  clone(replacementMemories) {
    const newMemory = new _WorkingMemory({
      soulName: this.soulName,
      memories: replacementMemories || this.memories,
      postCloneTransformation: this._postCloneTransformation,
      processor: this.processor
    });
    return this._postCloneTransformation(newMemory);
  }
  /**
   * Replaces the current memories in the WorkingMemory instance with new ones provided by the caller.
   * This method is nearly an alias of the `clone` method, with the key difference being that `replacementMemories` are required.
   * 
   * @param replacementMemories - An array of InputMemory objects to replace the current memories.
   * @returns A new WorkingMemory instance, with the memories replaced by the provided ones.
   * 
   * @example
   * ```
   * const newMemories = [{...}, {...}];
   * const updatedMemory = workingMemory.replace(newMemories);
   * ```
   */
  replace(replacementMemories) {
    return this.clone(replacementMemories);
  }
  /**
   * Applies a provided function to each memory in the WorkingMemory instance, producing a new WorkingMemory instance.
   * This method behaves similarly to the Array.prototype.map function, with the key difference being that it returns
   * a new immutable WorkingMemory instance containing the transformed memories, rather than an array of the transformed items.
   * 
   * @param callback - A function that accepts up to two arguments. The map method calls the callback function one time for each memory in the WorkingMemory.
   * @returns A new WorkingMemory instance with each memory transformed by the callback function.
   * 
   * @example
   * ```
   * const newWorkingMemory = workingMemory.map((memory, index) => {
   *   // Transform the memory here
   *   return transformedMemory;
   * });
   * ```
   */
  map(callback) {
    const unfrozenMemories = this.memories.map((memory) => {
      return {
        ...memory
      };
    });
    const newMemories = unfrozenMemories.map(callback);
    return this.clone(newMemories);
  }
  /**
   * Applies a provided asynchronous function to each memory in the WorkingMemory instance, producing a new WorkingMemory instance.
   * This method is similar to the `map` method but allows for asynchronous transformations of each memory. It returns
   * a new immutable WorkingMemory instance containing the transformed memories, rather than an array of the transformed items.
   * 
   * @param callback - An asynchronous function that accepts a memory and optional index (number). The asyncMap method calls the callback function one time for each memory in the WorkingMemory.
   *                   This function should return a Promise that resolves to the transformed memory.
   * @returns A Promise that resolves to a new WorkingMemory instance with each memory transformed by the asynchronous callback function.
   * 
   * @example
   * ```
   * const newWorkingMemory = await workingMemory.asyncMap(async (memory, index) => {
   *   // Asynchronously transform the memory here
   *   return await transformMemoryAsync(memory);
   * });
   * ```
   */
  async asyncMap(callback) {
    const newMemories = await Promise.all(this.memories.map(callback));
    return this.clone(newMemories);
  }
  /**
   * Returns a new WorkingMemory object with the memories sliced from `start` to `end` (`end` not included)
   * where `start` and `end` represent the index of items in the WorkingMemory's internal memory array. It behaves similarly to the `slice()` method of JavaScript arrays.
   * 
   * @param start - Zero-based index at which to start extraction. A negative index can be used, indicating an offset from the end of the sequence.
   * @param end - Zero-based index before which to end extraction. `slice` extracts up to but not including `end`. A negative index can be used, indicating an offset from the end of the sequence.
   * @returns A new WorkingMemory instance containing the extracted memories.
   * 
   * @example
   * ```
   * const slicedWorkingMemory = workingMemory.slice(1, 3);
   * ```
   */
  slice(start, end) {
    return this.clone(this.internalMemories.slice(start, end));
  }
  /**
   * Adds a single memory to the current set of memories in the WorkingMemory instance, producing a new WorkingMemory instance.
   * 
   * @param memory - The memory to add to the WorkingMemory.
   * @returns A new WorkingMemory instance with the added memory.
   * 
   * @example
   * ```
   * const newMemory = { role: ChatMessageRoleEnum.User, content: "Hello, world!" };
   * const newWorkingMemory = workingMemory.withMemory(newMemory);
   * ```
   */
  withMemory(memory) {
    return this.concat(this.normalizeMemoryListOrWorkingMemory([memory]));
  }
  /**
   * Filters the memories in the WorkingMemory instance using the provided callback, similar to Array.prototype.filter.
   * This method creates a new WorkingMemory instance with all memories that pass the test implemented by the provided function.
   * 
   * @param callback - A function that accepts a memory and returns a boolean. If it returns true, the memory is included in the new WorkingMemory instance.
   * @returns A new WorkingMemory instance with the filtered memories.
   */
  filter(callback) {
    const newMemories = this.memories.filter(callback);
    return this.clone(newMemories);
  }
  /**
   * Tests whether at least one memory in the WorkingMemory instance passes the test implemented by the provided callback, similar to Array.prototype.some.
   * This method does not modify the WorkingMemory instance.
   * 
   * @param callback - A function that accepts a memory and returns a boolean.
   * @returns A boolean indicating whether at least one memory passes the test.
   */
  some(callback) {
    return this.internalMemories.some(callback);
  }
  /**
   * Finds the first memory in the WorkingMemory instance that satisfies the provided testing function, similar to Array.prototype.find.
   * 
   * @param callback - A function that accepts a memory and returns a boolean. If it returns true, the memory is returned from the method.
   * @returns The first memory that satisfies the provided testing function, or undefined if no such memory is found.
   */
  find(callback) {
    const mem = this.memories.find(callback);
    if (!mem) {
      return mem;
    }
    return { ...mem };
  }
  /**
   * Concatenates the memories of another WorkingMemory (or an array of Memory objects) to the memories of the current WorkingMemory instance, similar to Array.prototype.concat.
   * This method creates a new WorkingMemory instance with the concatenated memories.
   * 
   * @param other - Another WorkingMemory (or an array of Memory innstances) to be concatenate with the current instance.
   * @returns A new WorkingMemory instance with the concatenated memories.
   */
  concat(other) {
    const otherWorkingMemory = this.normalizeMemoryListOrWorkingMemory(other);
    return this.clone(this.internalMemories.concat(otherWorkingMemory.memories));
  }
  /**
   * Prepends the memories the memories of another WorkingMemory (or an array of Memory objects) to the memories of the current WorkingMemory instance,
   * This method creates a new WorkingMemory instance with the memories of the other instance followed by the current instance's memories, similar to using WorkingMemory#concat in reverse.
   * 
   * @param otherWorkingMemory - Another MemoryListOrWorkingMemory instance whose memories are to be prepended to the current instance.
   * @returns A new WorkingMemory instance with the prepended memories.
   */
  prepend(otherWorkingMemory) {
    const otherMemory = this.normalizeMemoryListOrWorkingMemory(otherWorkingMemory);
    return this.clone(otherMemory.memories.concat(this.memories));
  }
  /**
   * Adds a monologue memory with the role of Assistant and the provided content to the WorkingMemory instance.
   * This method creates a new WorkingMemory instance with the added monologue memory.
   * 
   * @param content - The content of the monologue memory to add.
   * @returns A new WorkingMemory instance with the added monologue memory.
   */
  withMonologue(content) {
    return this.withMemory({
      role: "assistant" /* Assistant */,
      content
    });
  }
  async transform(transformation, opts = {}) {
    await this.finished;
    const newMemory = this.clone();
    newMemory.markPending();
    return newMemory.doTransform(transformation, opts);
  }
  /**
   * Returns a string representation of the internal memories of the WorkingMemory instance.
   * This method formats the internal memories into a readable string, showcasing each memory in a JSON stringified format.
   * 
   * @returns A string that represents the internal memories of the WorkingMemory instance.
   */
  toString() {
    return indentNicely`
      Working Memory (${this.id}): ${this.soulName}
      Memories:
      ${this.internalMemories.map((memory) => {
      return JSON.stringify(memory);
    }).join("\n")}
    `;
  }
  markPending() {
    const pendingInfo = this._pending();
    if (pendingInfo.pending) {
      throw new Error("attempting to mark pending a working memory already marked as pending");
    }
    pendingInfo.pending = new Promise((res) => {
      pendingInfo.pendingResolve = res;
    });
  }
  resolvePending() {
    const pendingInfo = this._pending();
    if (!pendingInfo.pendingResolve) {
      throw new Error("attempting to resolve pending on a memory that is not pending");
    }
    pendingInfo.pendingResolve();
    pendingInfo.pending = void 0;
    pendingInfo.pendingResolve = void 0;
  }
  async doTransform(transformation, opts) {
    if (!this._pending().pending) {
      throw new Error("attempting to update working memory not marked as pending");
    }
    try {
      const {
        postProcess = defaultPostProcessor,
        command,
        schema,
        skipAutoSchemaAddition,
        streamProcessor
      } = transformation;
      const processorSpec = opts.processor || this.processor;
      const processor = getProcessor(processorSpec.name, processorSpec.options);
      const commandMemory = typeof command === "string" ? {
        role: "system" /* System */,
        content: command
      } : command(this);
      if (schema && !skipAutoSchemaAddition) {
        commandMemory.content += "\n\n" + indentNicely`
          Respond *only* in JSON, conforming to the following JSON schema:
          ${JSON.stringify(zodToJsonSchema(schema), null, 2)}
        `;
      }
      const memoryWithCommand = this.withMemory(commandMemory);
      const response = await processor.process({
        memory: memoryWithCommand,
        schema,
        ...opts
      });
      if (opts.stream) {
        const valuePromise = new Promise(async (resolve, reject) => {
          try {
            const [memory2, value2] = await postProcess(this, await response.parsed);
            this.internalMemories.push(...this.memoriesFromInputMemories([memory2]));
            const usageNumbers2 = await response.usage;
            const usageObj2 = this._usage();
            Object.entries(usageNumbers2).forEach(([key, value3]) => {
              usageObj2[key] = value3;
            });
            resolve(value2);
          } catch (err) {
            reject(err);
          } finally {
            this.resolvePending();
          }
        });
        const stream = streamProcessor ? await streamProcessor(this, response.stream) : response.stream;
        return [this, stream, valuePromise];
      }
      const [memory, value] = await postProcess(this, await response.parsed);
      this.internalMemories.push(...this.memoriesFromInputMemories([memory]));
      const usageNumbers = await response.usage;
      const usageObj = this._usage();
      Object.entries(usageNumbers).forEach(([key, value2]) => {
        usageObj[key] = value2;
      });
      this.resolvePending();
      return [this, value];
    } catch (err) {
      console.error("error in doTransform", err);
      this.resolvePending();
      throw err;
    }
  }
  memoriesFromInputMemories(memories) {
    return memories.map((memory) => {
      return {
        ...memory,
        _id: memory._id || nanoid(),
        _timestamp: memory._timestamp || Date.now()
      };
    });
  }
  normalizeMemoryListOrWorkingMemory(memories) {
    if (memories instanceof _WorkingMemory) {
      return memories;
    }
    return this.clone(memories);
  }
};

// src/sharedTypes/eventLog.ts
var SoulEventKinds = /* @__PURE__ */ ((SoulEventKinds2) => {
  SoulEventKinds2["Perception"] = "perception";
  SoulEventKinds2["InteractionRequest"] = "interactionRequest";
  SoulEventKinds2["System"] = "system";
  return SoulEventKinds2;
})(SoulEventKinds || {});
var eventLogShape = {
  events: [],
  metadata: {},
  pendingToolCalls: {}
};
var debugChatShape = {
  metadata: {},
  state: {},
  eventLog: {}
};
export {
  AnthropicProcessor,
  ChatMessageRoleEnum,
  OpenAIProcessor,
  SoulEventKinds,
  WorkingMemory,
  createCognitiveStep,
  debugChatShape,
  eventLogShape,
  extractJSON,
  forkStream,
  getProcessor,
  indentNicely,
  prepareMemoryForJSON,
  registerProcessor,
  stripEntityAndVerb,
  stripEntityAndVerbFromStream,
  z
};
//# sourceMappingURL=index.mjs.map
